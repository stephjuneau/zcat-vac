{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f46f80a-ac4e-4ff7-95a6-f5aa0123408b",
   "metadata": {},
   "source": [
    "# Create redshift summary `zall` value added catalog: Tile-Cumulative Version\n",
    "StÃ©phanie Juneau (NOIRLab)\n",
    "\n",
    "## Summary\n",
    "\n",
    "Notebook used to create the `ztile` VAC (`zall-tilecumulative-edr-vac.fits`), which supersedes the redshift catalog `zall-tilecumulative-fuji.fits`. The EDR redshift VACs are described here: TODO: link to data doc website\n",
    "\n",
    "### Steps\n",
    "- Read in redshift catalog (`zall-tilecumulative-fuji.fits`)\n",
    "- Make a list of all SURVEYs, PROGRAMs\n",
    "- Iterate over each SURVEY-PROGRAM combination and create a file with the following coadded FIBERMAP quantities\n",
    "    - MEAN_FIBER_X, Y, RA, DEC\n",
    "    - STD_FIBER_RA, DEC\n",
    "    - MEAN_DELTA_X, Y\n",
    "    - RMS_DELTA_X, Y\n",
    "    - MIN_, MEAN_, MAX_MJD\n",
    "    - FIRSTNIGHT, LASTNIGHT\n",
    "- Assemble the patch files together to update values of existing columns and add four new columns: MIN_, MEAN, MAX_MJD, FIRSTNIGHT\n",
    "- Save the resulting VAC\n",
    "\n",
    "*Important note*: The `LASTNIGHT` column already exists and referts to the last NIGHT (KPNO calendar date) during whcih the tile (TILEID) was observed regardless if some TARGETIDs might be missing due to hardware problems in early phases of the survey validation (occurred in SV1).\n",
    "\n",
    "## Software\n",
    "\n",
    "This notebook can be run with DESI 22.2, 22.5 or 23.1 kernels. However, it is important to note that it behaves the same as `desispec/0.59.0` in terms of the decisions made when coadding FIBERMAP information from individual exposures (see [desispec.coaddition.coadd_fibermap()](https://github.com/desihub/desispec/blob/0.59.0/py/desispec/coaddition.py))\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3b51a3-2744-4932-91c0-7ef58340e9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from astropy.table import Table, join, vstack\n",
    "from astropy.io import fits\n",
    "\n",
    "# For FIBERSTATUS\n",
    "from desispec.fiberbitmasking import get_all_fiberbitmask_with_amp, get_all_nonamp_fiberbitmask_val, get_justamps_fiberbitmask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e6413-98f1-489e-a9f7-f1b5aaae878d",
   "metadata": {},
   "source": [
    "## Initial Setup (NERSC or Astro Data Lab?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fcdcc99-ec6d-4030-9d68-a87bbf8edb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = 'datalab'\n",
    "#platform = 'nersc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f552b249-47ed-4b85-a4a1-d4aadc38effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NERSC is the primary platform to access DESI files (below for DESI members)\n",
    "if platform=='nersc' :\n",
    "    desi_dir = \"/global/cfs/cdirs/desi/\"\n",
    "    \n",
    "    # Ouput directory for VAC (tests/ copy to final location after vetting)\n",
    "    vac_dir = desi_dir+\"science/gqp/vac/edr/zcat/tests/\"\n",
    "    \n",
    "    # Temporary directory for storing temporary patched coadded fibermap files\n",
    "    tmp_dir = desi_dir+\"science/gqp/vac/edr/zcat/tmp_patch/\"\n",
    "    \n",
    "# At Data Lab, using relative directories (available to DESI members for testing upon request)\n",
    "# (primary access to DESI EDR catalogs for users at Data Lab is via the databases)\n",
    "if platform=='datalab' :\n",
    "    \n",
    "    desi_dir = \"../../../../../DESI/\"\n",
    "\n",
    "    # Ouput directory for VAC\n",
    "    vac_dir = \"../../fuji/\"\n",
    "    \n",
    "    # Temporary directory for storing temporary patched coadded fibermap files\n",
    "    tmp_dir = \"../../fuji/tmp_patch/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63bb24a-376c-41d5-aba8-3c3c3f94fb43",
   "metadata": {},
   "source": [
    "## TILE coadd redshift catalog: `zall-tilecumulative`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd53fdcc-6abd-4558-947a-be3a5d681eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all combinations for fuji\n",
    "surveys_all = ['cmx', 'special',  'sv1',  'sv1',  'sv1',  'sv1',  'sv2',  'sv2',  'sv2',  'sv3',  'sv3',  'sv3']\n",
    "programs_all = ['other', 'dark', 'backup', 'bright', 'dark', 'other', 'backup', 'bright', 'dark', 'backup', 'bright', 'dark']\n",
    "\n",
    "N = len(surveys_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f209413-fccc-46a5-863a-011713a3e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original zcatalog\n",
    "zall_orig_file = desi_dir+\"spectro/redux/fuji/zcatalog/zall-tilecumulative-fuji.fits\"\n",
    "\n",
    "# New zcat VAC\n",
    "zvac_file = vac_dir+\"zall-tilecumulative-edr-vac.fits\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2967e3-a724-4d68-8b3e-b9163005c41f",
   "metadata": {},
   "source": [
    "## Coadded fibermap patch for each {survey}-{program}\n",
    "\n",
    "This handles separately a problematic duplicated tile (`TILEID=80870`) with two different values of `LASTNIGHT`. This special consideration is not needed for `zpix`, only for `ztile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebce2d9f-dc97-4738-a8d6-5a5975498ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coadd_fibermap_table(survey, program, fibermap_input, Ntruth):\n",
    "\n",
    "    print(f\"SURVEY={survey}; PROGRAM={program}\")\n",
    "    Nuniq = len(np.unique(fibermap_input['TARGETID','TILEID']))\n",
    "    if (survey!='sv1' or program!='other') and Nuniq!=Ntruth:\n",
    "        msg = \"Incompatible number\"\n",
    "        raise ValueError(msg)\n",
    "    \n",
    "    # some cols get combined into mean or rms ('FIBER_X', 'FIBER_Y' for tile-coadds *only*)\n",
    "    mean_cols = [\n",
    "        'DELTA_X', 'DELTA_Y', \n",
    "        'FIBER_X', 'FIBER_Y',\n",
    "        'PSF_TO_FIBER_SPECFLUX',\n",
    "        'MJD']\n",
    "\n",
    "    #- treat fiber coordinates separately due to bug w/ FIBER_RA==FIBER_DEC==0\n",
    "    mean_coords = ['FIBER_RA', 'FIBER_DEC']\n",
    "\n",
    "    #- rms_cols and std_cols must also be in mean_cols\n",
    "    rms_cols = ['DELTA_X', 'DELTA_Y']\n",
    "    std_cols = ['FIBER_RA', 'FIBER_DEC']\n",
    "        \n",
    "    #- Only a subset of \"good\" FIBERSTATUS flags are included in the coadd\n",
    "    fiberstatus_nonamp_bits = get_all_nonamp_fiberbitmask_val()\n",
    "    fiberstatus_amp_bits = get_justamps_fiberbitmask()\n",
    "    \n",
    "    #- check if any of the bad fiberstatus are set ignoring the amps for now\n",
    "    # (also ignoring warnings such as RESTRICTED, STUCKPOSITIONER, POORPOSITION)\n",
    "    nonamp_fiberstatus_flagged = ((fibermap_input['FIBERSTATUS'] & fiberstatus_nonamp_bits) > 0 )\n",
    "    #- check is all 3 amps are bad as coadd(spectra) works on individual amp/arm so any good amp can be used\n",
    "    allamps_flagged = ( (fibermap_input['FIBERSTATUS'] & fiberstatus_amp_bits) == fiberstatus_amp_bits )\n",
    "\n",
    "    # Add column with whether to keep this entry in the COADD (Bool)\n",
    "    fibermap_input['good_coadds'] = np.bitwise_not( nonamp_fiberstatus_flagged | allamps_flagged )    \n",
    " \n",
    "    # Create grouped tables --> quantities for the good FIBERSTATUS given priority \n",
    "    # --> When they don't exist (all FIBERSTATUS are bad) quantities are computed over all rejects \n",
    "    # Solution: groupby('TARGETID','good_coadds') and inverse-order so that True comes before False   \n",
    "\n",
    "    # Group joined table per TARGETID-TILEID (all rows)\n",
    "    fmap_group_all = fibermap_input.group_by(['TARGETID','TILEID'])\n",
    "\n",
    "    # Group joined table per TARGETID-TILEID and per good_coadds status\n",
    "    fmap_group = fibermap_input.group_by(['TARGETID','TILEID','good_coadds'])\n",
    " \n",
    "    #- mean cols (except for coordinate special case)\n",
    "    t_mean = fmap_group[['TARGETID','TILEID','good_coadds']+mean_cols].groups.aggregate(np.mean)\n",
    "    for col in ['FIBER_X','FIBER_Y','DELTA_X','DELTA_Y','PSF_TO_FIBER_SPECFLUX']: t_mean[col] = (t_mean[col]).astype(np.float32)\n",
    "    t_mean.rename_columns(mean_cols, ['MEAN_'+col for col in mean_cols])\n",
    "\n",
    "    #- min & max MJD cols\n",
    "    t_min = fmap_group['TARGETID','TILEID','good_coadds','MJD'].groups.aggregate(np.min)\n",
    "    t_min.rename_column('MJD', 'MIN_MJD')\n",
    "    t_max = fmap_group['TARGETID','TILEID','good_coadds','MJD'].groups.aggregate(np.max)\n",
    "    t_max.rename_column('MJD', 'MAX_MJD')\n",
    "\n",
    "    #-first & last NIGHT cols (*not* using good_coadds condition to count all exposures)\n",
    "    t_first = fmap_group_all['TARGETID','TILEID','NIGHT'].groups.aggregate(np.min)\n",
    "    t_first.rename_column('NIGHT', 'FIRSTNIGHT')\n",
    "    t_last = fmap_group_all['TARGETID','TILEID','NIGHT'].groups.aggregate(np.max)\n",
    "    t_last.rename_column('NIGHT', 'LASTNIGHT')\n",
    "    \n",
    "    #- rms (root mean square) cols: rms = sqrt(mean(x**2))\n",
    "    fmap_rms = fmap_group[['TARGETID','TILEID','good_coadds']+rms_cols]\n",
    "    for col in rms_cols: fmap_rms[col] = fmap_rms[col]**2  #square\n",
    "    t_rms = fmap_rms[['TARGETID','TILEID','good_coadds']+rms_cols].groups.aggregate(np.mean)  #mean\n",
    "    for col in rms_cols: t_rms[col] = (np.sqrt(t_rms[col])).astype(np.float32)  #root\n",
    "    \n",
    "    t_rms.rename_columns(rms_cols, ['RMS_'+col for col in rms_cols])\n",
    "\n",
    "    #- Combine results so far:\n",
    "    t_join = t_mean\n",
    "    t_join['SURVEY'] = survey\n",
    "    t_join['PROGRAM'] = program\n",
    "    t_join = join(t_join, t_rms, join_type='left', keys=['TARGETID','TILEID','good_coadds'])\n",
    "    t_join = join(t_join, t_min, join_type='left', keys=['TARGETID','TILEID','good_coadds'])\n",
    "    t_join = join(t_join, t_max, join_type='left', keys=['TARGETID','TILEID','good_coadds'])\n",
    "    \n",
    "    #- Sort on TARGETID and reverse of good_coadds (to keep True=1 before False=0)\n",
    "    t_join.sort(keys=['TARGETID','TILEID','good_coadds'], reverse=True)\n",
    "    u, indices = np.unique(t_join['TARGETID','TILEID'], return_index=True)\n",
    "    t_uniq = t_join[indices]\n",
    "    t_uniq.rename_column('good_coadds', 'COADD_EXIST')\n",
    "    \n",
    "    #- Check if can join also with first,last night\n",
    "    if len(t_uniq)==len(t_first):\n",
    "        t_uniq = join(t_uniq, t_first, join_type='left', keys=['TARGETID','TILEID'])\n",
    "        t_uniq = join(t_uniq, t_last, join_type='left', keys=['TARGETID','TILEID'])\n",
    "    else:\n",
    "        msg = f'ERROR: Inconsistent number of rows in FIRST/LAST table: found {len(t_first)} but expecting {len(t_uniq)}'\n",
    "        raise ValueError(msg)        \n",
    "    \n",
    "    #- Sanity check if reading a coadded extension (will treat special case SV1-other with duplicated tile below)\n",
    "    if (survey!='sv1' or program!='other') and len(t_uniq)!=Ntruth:\n",
    "        msg = f'ERROR: Inconsistent number of rows in MEAN+MIN+MAX table: found {len(t_uniq)} but expecting {Ntruth}'\n",
    "        raise ValueError(msg)        \n",
    "\n",
    "    # Identify cases with missing coords to group them separately\n",
    "    missing_coord = (fibermap_input['FIBER_RA']==0.)&(fibermap_input['FIBER_DEC']==0.)\n",
    "    valid_coord = ~missing_coord\n",
    "\n",
    "    #- Group again without the missing FIBER_RA/DEC\n",
    "    fmap_select = fibermap_input[valid_coord]\n",
    "    fmap_valid = fmap_select.group_by(['TARGETID','TILEID','good_coadds'])\n",
    "    \n",
    "    #- Special case of averaging coordinates without missing FIBER_RA/DEC    \n",
    "    t_mean_coords = fmap_valid[['TARGETID','TILEID','good_coadds']+mean_coords].groups.aggregate(np.mean)\n",
    "    t_mean_coords.rename_columns(mean_coords, ['MEAN_'+col for col in mean_coords])\n",
    "\n",
    "    #- std cols: std = sqrt(mean([x-mean(x)]**2))\n",
    "    fmap_std = fmap_valid[['TARGETID','TILEID','good_coadds']+std_cols]\n",
    "    mean_values = join(fmap_std, t_mean_coords, join_type='left', keys=['TARGETID','TILEID','good_coadds'])\n",
    "\n",
    "    for col in std_cols: \n",
    "        fmap_std[col] = (fmap_std[col] - mean_values['MEAN_'+col])**2  #square of (x-mean(x))\n",
    "\n",
    "    t_std_coords = fmap_std[['TARGETID','TILEID','good_coadds']+std_cols].groups.aggregate(np.mean)  #mean\n",
    "    for col in std_cols: t_std_coords[col] = np.sqrt(t_std_coords[col])  #root\n",
    "\n",
    "    t_std_coords.rename_columns(std_cols, ['STD_'+col for col in std_cols])\n",
    "\n",
    "    # Convert from degrees to arcseconds with cos(dec) term\n",
    "    t_std_coords['STD_FIBER_RA'] = (t_std_coords['STD_FIBER_RA']*3600.*np.cos(np.radians(t_std_coords['STD_FIBER_DEC']))).astype(np.float32)\n",
    "    t_std_coords['STD_FIBER_DEC'] = (t_std_coords['STD_FIBER_DEC']*3600.).astype(np.float32)\n",
    "    \n",
    "    #- Group together new FIBER coords tables and create unique table again\n",
    "    t_join_coords = join(t_mean_coords, t_std_coords, join_type='left', keys=['TARGETID','TILEID','good_coadds'])\n",
    "    \n",
    "    #- Sort on TARGETID and reverse of good_coadds (to keep True=1 before False=0)\n",
    "    t_join_coords.sort(keys=['TARGETID','TILEID','good_coadds'], reverse=True)\n",
    "    u, indices = np.unique(t_join_coords['TARGETID','TILEID'], return_index=True)\n",
    "    t_uniq_coords = t_join_coords[indices]\n",
    "    t_uniq_coords.remove_column('good_coadds')\n",
    "    \n",
    "    #- Sanity check\n",
    "    if (survey!='sv1' or program!='other') and len(t_uniq_coords)!=Ntruth:\n",
    "        msg = f'ERROR: Inconsistent number of rows in FIBER COORDS table: found {len(t_uniq_coords)} but expecting {Ntruth}'\n",
    "        raise ValueError(msg)    \n",
    "    \n",
    "    #- Create output table by joining all the new columns so far\n",
    "    t_out = join(t_uniq, t_uniq_coords, keys=['TARGETID','TILEID'])\n",
    "    \n",
    "    #- Add SURVEY, PROGRAM columns because these came from {survey}-{program} files\n",
    "    t_out['SURVEY']=survey\n",
    "    t_out['PROGRAM']=program\n",
    "\n",
    "    return(t_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0877843-a460-4960-98fe-d7c313e32f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dup_tile(fm, cat):\n",
    "        \n",
    "    early_night = 20210512\n",
    "    keep = fm['NIGHT']<=early_night\n",
    "    fm = fm[keep]\n",
    "    \n",
    "    N_cat = len(cat)\n",
    "    \n",
    "    survey = 'sv1'\n",
    "    program = 'other'\n",
    "    \n",
    "    t_out_dup = coadd_fibermap_table(survey, program, fm, N_cat)\n",
    "    print(f\"Returning a table with N(rows)={len(t_out_dup)}\")\n",
    "        \n",
    "    return(t_out_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b06861f-fd01-4564-ba2c-cabb4fa278a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop over survey-program\n",
    "\n",
    "for i in np.arange(N):\n",
    "\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    survey = surveys_all[i]\n",
    "    program = programs_all[i]\n",
    "\n",
    "    # Read file with two extensions per each survey-program\n",
    "    zfile = f\"{desi_dir}spectro/redux/fuji/zcatalog/ztile-{survey}-{program}-cumulative.fits\"\n",
    "\n",
    "    patch_file = f\"{tmp_dir}ztile-{survey}-{program}-coaddpatch.fits\"\n",
    "    \n",
    "    # By default, only generate the patch files once and skip if they exist (this is a time intensive step)\n",
    "    if os.path.isfile(patch_file):\n",
    "        continue\n",
    "    \n",
    "    print(f\"Reading file: ztile-{survey}-{program}-cumulative.fits\") \n",
    "    \n",
    "    # Read the coadded extension just to keep the unique identifiers for testing/sanity checks\n",
    "    zcat_single = Table.read(zfile, \"ZCATALOG\")\n",
    "    zcat_truth = zcat_single['TARGETID','TILEID','LASTNIGHT']\n",
    "    del zcat_single\n",
    "    \n",
    "    # Read the exp_fibermap to use for calculations\n",
    "    fibermap_input = Table.read(zfile, \"EXP_FIBERMAP\")\n",
    "    \n",
    "    # Keep only rows with TARGETID>0\n",
    "    zcat_truth = zcat_truth[zcat_truth['TARGETID']>0]\n",
    "    fibermap_input = fibermap_input[fibermap_input['TARGETID']>0]\n",
    "\n",
    "    # Number of expected rows to cross-check later\n",
    "    Ntruth = len(zcat_truth)\n",
    "    \n",
    "    #- Call the coadd_fibermap_table() function\n",
    "    t_out = coadd_fibermap_table(survey, program, fibermap_input, Ntruth)\n",
    "    \n",
    "    #- Sanity check for all regular cases\n",
    "    if (survey!='sv1' or program!='other') and len(t_out)!=Ntruth:\n",
    "        msg = f'ERROR: Inconsistent number of rows in OUTPUT table: found {len(t_out)} but expecting {Ntruth}'\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    #- Sanity check and dealing with the exception duplicate tile\n",
    "    if len(t_out)!=Ntruth and survey=='sv1' and program=='other' :\n",
    "        # Tile with Duplicate LASTNIGHT\n",
    "        duptile = 80870\n",
    "        fm_dup = fibermap_input[fibermap_input['TILEID']==duptile]\n",
    "        cat_dup = t_out[t_out['TILEID']==duptile]\n",
    "            \n",
    "        print(f\"BEFORE the fix = {len(t_out)}\")\n",
    "            \n",
    "        # Fix the known issue with duplicate TILEID in sv1-other with supplement for extra LASTNIGHT\n",
    "        t_out_dup = fix_dup_tile(fm_dup, cat_dup)\n",
    "        print(f\"... supplement table = {len(t_out_dup)}\")\n",
    "             \n",
    "        t_out = vstack([t_out, t_out_dup])\n",
    "        print(f\"AFTER the fix = {len(t_out)}\")\n",
    "            \n",
    "        # Check again to make sure the fix worked\n",
    "        if len(t_out)!=Ntruth:\n",
    "            msg = f'ERROR: Inconsistent number of rows in OUTPUT table for SV1-other: found {len(t_out)} but expecting {Ntruth}'\n",
    "            raise ValueError(msg)\n",
    "        \n",
    "    #- Save output file per {survey}-{program}\n",
    "    t_out.write(patch_file, overwrite=True)\n",
    "    print(f\"Wrote output file: {patch_file}\")\n",
    "   \n",
    "    # End timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = np.round(end_time - start_time, 2)\n",
    "    print(\"----------------- Elapsed time [sec]: \", elapsed_time)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed5eb79c-b102-44a7-8b46-77421f81cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory (only if actually running the patch files)\n",
    "#del fibermap_input\n",
    "#del t_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d6284-6776-4c35-a969-fcea267c23fa",
   "metadata": {},
   "source": [
    "## Original zall file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c144a0d3-d041-4bf2-bb1d-05ed5535710b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: ../../../../../DESI/spectro/redux/fuji/zcatalog/zall-tilecumulative-fuji.fits\n",
      "No.    Name      Ver    Type      Cards   Dimensions   Format\n",
      "  0  PRIMARY       1 PrimaryHDU       4   ()      \n",
      "  1  ZCATALOG      1 BinTableHDU    357   3611000R x 141C   [K, 7A, 6A, J, J, D, D, K, D, 10D, K, 6A, 20A, K, D, I, J, K, J, J, D, D, E, E, E, E, K, B, 3A, E, E, J, D, J, I, 8A, J, J, 4A, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, I, E, E, E, E, K, 2A, E, E, E, E, 1A, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, K, D, D, J, I, E, I, I, E, E, E, E, D, E, D, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, J, L, K, L]   \n"
     ]
    }
   ],
   "source": [
    "fits.info(zall_orig_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42d208d5-8686-4940-a225-8e6cb4d44095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 s, sys: 2.55 s, total: 14.6 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read input catalog and only keep cases with TARGETID>0\n",
    "tz = Table.read(zall_orig_file)\n",
    "tz = tz[tz['TARGETID']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad12a9c-54c6-496b-a264-c48e38c8fd78",
   "metadata": {},
   "source": [
    "## Read in patch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8b24ca8-eb8d-487c-b516-43307340053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dup_tile(arr_in):\n",
    "    tt = Table(arr_in)\n",
    "    print(f\"Table with N rows = {len(tt)}\")\n",
    "    \n",
    "    u, indices, counts = np.unique(tt['TILEID'],return_index=True, return_counts=True)\n",
    "    checktile = u[counts>1]\n",
    "    \n",
    "    out = tt[np.isin(tt['TILEID'], checktile)]\n",
    "    print(f\"Num rows with duplicated tiles = {len(out)}\")\n",
    "    \n",
    "    for tile in out['TILEID']:\n",
    "        print(f\"Tile {tile} has LASTNIGHT=\", out['LASTNIGHT'][out['TILEID']==tile])\n",
    "        \n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "427793b9-543f-4936-a0db-7b505df88ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================\n",
      "SURVEY=cmx; PROGRAM=other\n",
      "---- for patch ----\n",
      "5000\n",
      "1\n",
      "1\n",
      "---- for original ----\n",
      "5000\n",
      "5000\n",
      "1\n",
      "1\n",
      "=============================================================\n",
      "SURVEY=special; PROGRAM=dark\n",
      "---- for patch ----\n",
      "67201\n",
      "16\n",
      "16\n",
      "---- for original ----\n",
      "67201\n",
      "67201\n",
      "16\n",
      "16\n",
      "=============================================================\n",
      "SURVEY=sv1; PROGRAM=backup\n",
      "---- for patch ----\n",
      "113845\n",
      "26\n",
      "27\n",
      "---- for original ----\n",
      "113845\n",
      "113845\n",
      "26\n",
      "26\n",
      "=============================================================\n",
      "SURVEY=sv1; PROGRAM=bright\n",
      "---- for patch ----\n",
      "239500\n",
      "50\n",
      "76\n",
      "---- for original ----\n",
      "239500\n",
      "239500\n",
      "50\n",
      "50\n",
      "=============================================================\n",
      "SURVEY=sv1; PROGRAM=dark\n",
      "---- for patch ----\n",
      "384000\n",
      "80\n",
      "95\n",
      "---- for original ----\n",
      "384000\n",
      "384000\n",
      "80\n",
      "80\n",
      "=============================================================\n",
      "SURVEY=sv1; PROGRAM=other\n",
      "---- for patch ----\n",
      "146304\n",
      "32\n",
      "44\n",
      "---- for original ----\n",
      "151304\n",
      "146304\n",
      "32\n",
      "33\n",
      "=============================================================\n",
      "SURVEY=sv2; PROGRAM=backup\n",
      "---- for patch ----\n",
      "4208\n",
      "1\n",
      "1\n",
      "---- for original ----\n",
      "4208\n",
      "4208\n",
      "1\n",
      "1\n",
      "=============================================================\n",
      "SURVEY=sv2; PROGRAM=bright\n",
      "---- for patch ----\n",
      "70337\n",
      "18\n",
      "18\n",
      "---- for original ----\n",
      "70337\n",
      "70337\n",
      "18\n",
      "18\n",
      "=============================================================\n",
      "SURVEY=sv2; PROGRAM=dark\n",
      "---- for patch ----\n",
      "84065\n",
      "20\n",
      "20\n",
      "---- for original ----\n",
      "84065\n",
      "84065\n",
      "20\n",
      "20\n",
      "=============================================================\n",
      "SURVEY=sv3; PROGRAM=backup\n",
      "---- for patch ----\n",
      "152740\n",
      "35\n",
      "35\n",
      "---- for original ----\n",
      "152740\n",
      "152740\n",
      "35\n",
      "35\n",
      "=============================================================\n",
      "SURVEY=sv3; PROGRAM=bright\n",
      "---- for patch ----\n",
      "907978\n",
      "214\n",
      "215\n",
      "---- for original ----\n",
      "907978\n",
      "907978\n",
      "214\n",
      "214\n",
      "=============================================================\n",
      "SURVEY=sv3; PROGRAM=dark\n",
      "---- for patch ----\n",
      "1027391\n",
      "239\n",
      "239\n",
      "---- for original ----\n",
      "1027391\n",
      "1027391\n",
      "239\n",
      "239\n"
     ]
    }
   ],
   "source": [
    "t_patch = Table()\n",
    "\n",
    "# N\n",
    "for i in np.arange(N):\n",
    "    survey = surveys_all[i]\n",
    "    program = programs_all[i]\n",
    "\n",
    "    # Read file per each survey-program\n",
    "    filename = f\"{tmp_dir}ztile-{survey}-{program}-coaddpatch.fits\"\n",
    "    t = Table.read(filename)\n",
    "    \n",
    "    t_patch = vstack([t_patch, t])\n",
    "    \n",
    "    print(\"=============================================================\")\n",
    "    print(f\"SURVEY={survey}; PROGRAM={program}\")\n",
    "    print(\"---- for patch ----\")\n",
    "    print(len(np.unique(t['TARGETID','TILEID'])))\n",
    "    print(len(np.unique(t['TILEID'])))\n",
    "    print(len(np.unique(t['TILEID','LASTNIGHT'])))\n",
    "#    check_cases = find_dup_tile(np.unique(t['TILEID','LASTNIGHT']))\n",
    "    \n",
    "    print(\"---- for original ----\")\n",
    "    tz_sel = tz[(tz['SURVEY']==survey)&(tz['PROGRAM']==program)]\n",
    "    print(len(np.unique(tz_sel['TARGETID','TILEID','LASTNIGHT'])))\n",
    "    print(len(np.unique(tz_sel['TARGETID','TILEID'])))\n",
    "    print(len(np.unique(tz_sel['TILEID'])))\n",
    "    print(len(np.unique(tz_sel['TILEID','LASTNIGHT'])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f85043c6-3a5d-4bc1-a376-4fff9f439da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For info / debugging\n",
    "#t_patch.colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41fe457e-71ce-43ee-8ae0-4e520e32c13f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3207569\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><i>Table length=5</i>\n",
       "<table id=\"table139836209899936\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>TARGETID</th><th>TILEID</th><th>COADD_EXIST</th><th>MEAN_DELTA_X</th><th>MEAN_DELTA_Y</th><th>MEAN_FIBER_X</th><th>MEAN_FIBER_Y</th><th>MEAN_PSF_TO_FIBER_SPECFLUX</th><th>MEAN_MJD</th><th>SURVEY</th><th>PROGRAM</th><th>RMS_DELTA_X</th><th>RMS_DELTA_Y</th><th>MIN_MJD</th><th>MAX_MJD</th><th>FIRSTNIGHT</th><th>LASTNIGHT</th><th>MEAN_FIBER_RA</th><th>MEAN_FIBER_DEC</th><th>STD_FIBER_RA</th><th>STD_FIBER_DEC</th></tr></thead>\n",
       "<thead><tr><th>int64</th><th>int32</th><th>bool</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float64</th><th>bytes7</th><th>bytes6</th><th>float32</th><th>float32</th><th>float64</th><th>float64</th><th>int32</th><th>int32</th><th>float64</th><th>float64</th><th>float32</th><th>float32</th></tr></thead>\n",
       "<tr><td>39628473198708395</td><td>80615</td><td>False</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.7702122</td><td>59200.095110125</td><td>cmx</td><td>other</td><td>0.0</td><td>0.0</td><td>59200.06640136</td><td>59200.12381137</td><td>20201216</td><td>20201216</td><td>23.6619676773673</td><td>29.8475887928968</td><td>0.0</td><td>0.0</td></tr>\n",
       "<tr><td>39628473198709499</td><td>80615</td><td>True</td><td>-0.0055</td><td>-0.00375</td><td>69.6375</td><td>-391.3065</td><td>0.7471383</td><td>59200.095110125</td><td>cmx</td><td>other</td><td>0.008093207</td><td>0.0124197425</td><td>59200.06640136</td><td>59200.12381137</td><td>20201216</td><td>20201216</td><td>23.711789506441093</td><td>29.843712547212316</td><td>0.1003415</td><td>0.15683237</td></tr>\n",
       "<tr><td>39628473198710139</td><td>80615</td><td>True</td><td>-0.0085</td><td>-0.0035</td><td>62.71825</td><td>-382.719</td><td>0.7309045</td><td>59200.095110125</td><td>cmx</td><td>other</td><td>0.009949874</td><td>0.01317194</td><td>59200.06640136</td><td>59200.12381137</td><td>20201216</td><td>20201216</td><td>23.742668270905856</td><td>29.874927385824513</td><td>0.0867613</td><td>0.16850825</td></tr>\n",
       "<tr><td>39628473198710603</td><td>80615</td><td>True</td><td>-0.00675</td><td>-0.0055</td><td>58.0</td><td>-394.30325</td><td>0.71950334</td><td>59200.095110125</td><td>cmx</td><td>other</td><td>0.0091515025</td><td>0.014124447</td><td>59200.06640136</td><td>59200.12381137</td><td>20201216</td><td>20201216</td><td>23.764893506253426</td><td>29.832358617984326</td><td>0.10309766</td><td>0.1715876</td></tr>\n",
       "<tr><td>39628473198711006</td><td>80615</td><td>True</td><td>-0.002</td><td>-0.00875</td><td>53.11</td><td>-384.87424</td><td>0.789</td><td>59200.095110125</td><td>cmx</td><td>other</td><td>0.0068920245</td><td>0.01566046</td><td>59200.06640136</td><td>59200.12381137</td><td>20201216</td><td>20201216</td><td>23.786503075810064</td><td>29.866703322515196</td><td>0.10997444</td><td>0.17224741</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=5>\n",
       "     TARGETID     TILEID COADD_EXIST ... STD_FIBER_RA STD_FIBER_DEC\n",
       "      int64       int32      bool    ...   float32       float32   \n",
       "----------------- ------ ----------- ... ------------ -------------\n",
       "39628473198708395  80615       False ...          0.0           0.0\n",
       "39628473198709499  80615        True ...    0.1003415    0.15683237\n",
       "39628473198710139  80615        True ...    0.0867613    0.16850825\n",
       "39628473198710603  80615        True ...   0.10309766     0.1715876\n",
       "39628473198711006  80615        True ...   0.10997444    0.17224741"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(t_patch))\n",
    "t_patch[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6c09d2e-fda3-4067-a67f-d934d34243ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N rows in patch table = 3207569\n",
      "N with unqiue TARGETID,TILEID = 3202569\n",
      "N unique TILEIDs = 732\n",
      "N unique TILEID-LASTNIGHT = 787\n"
     ]
    }
   ],
   "source": [
    "print(f\"N rows in patch table = {len(t_patch)}\")\n",
    "print(f\"N with unqiue TARGETID,TILEID = {len(np.unique(t_patch['TARGETID','TILEID']))}\")\n",
    "print(f\"N unique TILEIDs = {len(np.unique(t_patch['TILEID']))}\")\n",
    "print(f\"N unique TILEID-LASTNIGHT = {len(np.unique(t_patch['TILEID','LASTNIGHT']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10c63a18-4daf-4d30-8f35-313c4d245e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N rows in zpix table = 3207569\n",
      "N with unqiue TARGETID,TILEID = 3202569\n",
      "N with unqiue TARGETID,TILEID, LASTNIGHT = 3207569\n",
      "N unique TILEIDs = 732\n",
      "N unique TILEID-LASTNIGHT = 733\n"
     ]
    }
   ],
   "source": [
    "print(f\"N rows in zpix table = {len(tz)}\")\n",
    "print(f\"N with unqiue TARGETID,TILEID = {len(np.unique(tz['TARGETID','TILEID']))}\")\n",
    "print(f\"N with unqiue TARGETID,TILEID, LASTNIGHT = {len(np.unique(tz['TARGETID','TILEID','LASTNIGHT']))}\")\n",
    "print(f\"N unique TILEIDs = {len(np.unique(tz['TILEID']))}\")\n",
    "print(f\"N unique TILEID-LASTNIGHT = {len(np.unique(tz['TILEID','LASTNIGHT']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6376a9-84b2-4fb6-b4e0-a7ae48489314",
   "metadata": {},
   "source": [
    "## Replace and add columns based on patch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1ec0e27-890e-4fe8-bc7c-afcbd307c865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 15s, sys: 1.31 s, total: 1min 17s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Check that lenghts and row order are consistent\n",
    "Nz = len(tz)\n",
    "Np = len(t_patch)\n",
    "\n",
    "# Columns exist but values are updated\n",
    "cols_to_replace = ['MEAN_FIBER_X', 'MEAN_FIBER_Y','MEAN_DELTA_X','RMS_DELTA_X','MEAN_DELTA_Y','RMS_DELTA_Y',\\\n",
    "                   'MEAN_FIBER_RA', 'STD_FIBER_RA', 'MEAN_FIBER_DEC', 'STD_FIBER_DEC','MEAN_PSF_TO_FIBER_SPECFLUX']\n",
    "\n",
    "# LASTNIGHT already exists in the zall-tilecumulative file (leaving it as is)\n",
    "cols_to_add = ['MIN_MJD', 'MEAN_MJD', 'MAX_MJD', 'FIRSTNIGHT']\n",
    "cols_to_fix = ['LASTNIGHT']\n",
    "\n",
    "\n",
    "if Nz==Np:\n",
    "    tz.sort(['SURVEY','PROGRAM','TILEID','TARGETID'])\n",
    "    t_patch.sort(['SURVEY','PROGRAM','TILEID','TARGETID'])\n",
    "    \n",
    "    # Fix LASTNIGHT to be able to use it properly\n",
    "    duptile = 80870\n",
    "    rfix_z = tz['TILEID']!=duptile\n",
    "    rfix_patch = t_patch['TILEID']!=duptile\n",
    "\n",
    "    t_patch['LASTNIGHT'][rfix_patch] = tz['LASTNIGHT'][rfix_z]\n",
    "\n",
    "    # Check that this worked\n",
    "    Nz_check = len(np.unique(tz['TARGETID','TILEID','LASTNIGHT']))\n",
    "    Npatch_check = len(np.unique(t_patch['TARGETID','TILEID','LASTNIGHT']))\n",
    "    if Nz_check!=Npatch_check:\n",
    "        msg = (f\"Uh oh the LASTNIGHT fix didn't work: got {Npatch_check} instead of {Nz_check}\")\n",
    "        raise ValueError(msg)\n",
    "    \n",
    "    # Re-sort with LASTNIGHT to take care of dup tile\n",
    "    tz.sort(['SURVEY','PROGRAM','TILEID','LASTNIGHT','TARGETID'])\n",
    "    t_patch.sort(['SURVEY','PROGRAM','TILEID','LASTNIGHT','TARGETID'])\n",
    "    \n",
    "    # Replace columns listed above\n",
    "    for col in cols_to_replace:\n",
    "        tz[col] = t_patch[col]\n",
    "\n",
    "    # Add new columns listed above\n",
    "    for col in cols_to_add:\n",
    "        tz[col] = t_patch[col]\n",
    "else:\n",
    "    print(f\"Different numbers: expecting {Nz} but the patch table has {Np}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03c71593-9728-4bac-b4f2-f497084cfac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3207569\n",
      "3207569\n",
      "3207569\n",
      "3207569\n",
      "3207569\n"
     ]
    }
   ],
   "source": [
    "print(len(tz))\n",
    "\n",
    "# Check on columns that had problems in the past due to missing coordinates (now should be ignored)\n",
    "finite_cols = ['MEAN_FIBER_RA', 'MEAN_FIBER_DEC', 'STD_FIBER_RA', 'STD_FIBER_DEC']\n",
    "for col in finite_cols:\n",
    "    print(len(tz[np.isfinite(tz[col])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cba5203-509d-4ba9-960e-4a39c87c1a67",
   "metadata": {},
   "source": [
    "## Save output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e650dad-bddc-4e3a-9dbe-cb66db7df108",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Will need to add overwirte=True if wants to overwrite an existing file\n",
    "tz.write(zvac_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DESI 23.1",
   "language": "python",
   "name": "desi_23.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
